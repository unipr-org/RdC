```table-of-contents
```
---

# Audio
La frequenza di campionamento è determinata dal teorema di Nyquist ([[1-livello_fisico]]).
La quantizzazione significa rappresentare un segnale analogico, come un'onda sonora, con un numero discreto di valori ad esempio:
- Un sistema a 8 bit offre $2^8$ = 256 possibili valori di quantizzazione.
- Un sistema a 16 bit offre $2^{16}$ = 65.536 possibili valori di quantizzazione.

---

## Compressione Audio
La compressione audio è un processo che riduce la dimensione dei file audio, eliminando o riducendo alcune informazioni superflue. Questo processo è utilizzato per vari scopi, inclusa la trasmissione su Internet.

La compressione audio si può fare in due modi:
- **Conversione per forma d’onda**: il segnale viene convertito usando la trasformata di Fourier, è una tecnica matematica che consente di decomporre un segnale nel dominio di tempo in componenti nel dominio delle frequenze, cioè il segnale viene quindi scomposto nelle sue frequenze costituenti.
    Dopo di che le componenti possono essere quantizzate, ovvero rappresentate con un numero discreto di valori.

- **Codifica percettiva**: sfrutta alcuni limiti del sistema uditivo umano (psicoacustica) per codificare il segnale in modo che sembri lo stesso ad un ascoltatore umano pur essendo diverso dal segnale originario, utilizzando la tecnica del mascheramento, ad esempio, eliminando o riducendo suoni più deboli che sono mascherati da quelli più forti, diminuendo la quantità complessiva di dati necessari per la rappresentazione del suono.

![[compressione_audio.png]]

>I formati MP3 e AAC eseguono la trasformata di Fourier, quindi codificano solo le frequenze non mascherabili.
>CD audio: si arriva a circa 100 Kbps (1.4Mbps non compresso, Compressione 14:1).

---

# Video digitale
Video digitale significa avere una sequenza di fotogrammi, ognuno dei quali composto da una griglia rettangolare di elementi detti **pixel**.
La geometria è quella del video analogico con la differenza che le linee di scansione vengono sostituite da righe di pixel discreti.

Per avere un movimento fluido servono almeno 25 fotogrammi al secondo, per un flusso video alla risoluzione di 640x480, con 24 bit per pixel e 30 fotogrammi al secondo serve una linea di comunicazione a 200 Mbps.

**Calcolo**
$$\text{Larghezza Di Banda = Risoluzione * Profondità Di Colore * FPS}$$

Dove:
- La **Risoluzione** è il numero totale di pixel in un singolo fotogramma, calcolato come Larghezza×Altezza.
- La **Profondità di colore** è il numero di bit utilizzati per rappresentare il colore di un singolo pixel.
- I **Fotogrammi al secondo** rappresentano il numero di fotogrammi trasmessi in un secondo.

Quindi:
- Risoluzione: 640x480
- Profondità di colore: 24
- Fotogrammi al secondo: 30

Soluzione:
$$\text{Larghezza di Banda} = 640 * 480 * 24 * 30 = 221.184.000 \;bps$$
$$\text{Larghezza di Banda} = 221.184.000 * 10^{-6} = 221.18Mbps ≈ 220Mbps$$

La compressione è l’unica possibilità per riuscire ad inviare filmati video su internet.

Servono due algoritmi: uno di codifica per la compressione all’origine, uno di decodifica per la decompressione alla destinazione

Per i sistemi non in tempo reale il sistema di codifica può essere lento e utilizzare hardware costoso a favore di un sistema di decodifica veloce ed economico

Il sistema di codifica/decodifica può non essere reversibile (lossy): a costo di una piccola perdita di informazioni si ottiene un fattore di compressione elevato

---

## JPEG
> Joint Photographic Experts Group.

Un video è essenzialmente una sequenza di immagini con audio.
Un algoritmo per la codifica video è un algoritmo che codifica in successione ogni singola immagine con o senza perdite di informazione.

Lo standard **JPEG** esegue la compressione di immagini statiche a toni continui.

![[JPEG.png]]

Esempio:
Supponiamo di lavorare con una immagine 640x480 con 24 bit per pixel:
1. **Preparazione del blocco**: si passa dalle coordinate RGB in coordinate Y (luminanza), Cb e Cr (Crominananza) attraverso una trasformazione lineare.
    Y = Blu + Rosso + Verde
    Cb = Blu - Y
    Cr = Rosso - Y
    ![[RGB.png]]

2. Si applica quindi una **DCT** (Trasformazione Discreta del Coseno) ad ognuno dei blocchi separatamente. 
    DCT e' simile alla DFT (rappresenta una variabile temporale nello spazio delle frequenze), ma usa solo numeri reali.
    ![[DCT.png]]

3. **Quantizzazione**: vengono eliminati i coefficienti DCT meno importanti. Questo comporta una perdita e avviene dividendo ogni coefficiente per un peso estratto da una tabella. Tale tabella non fa parte dello standard e ogni applicazione deve fornire i propri.
    ![[Quantizzazione.png]]

**Quantizzazione differenziale**: per ogni blocco, il valore (0,0) viene sostituito con la quantità di cui differisce dall’elemento corrispondente nel blocco precedente.

**Linearizzazione**: dei 64 elementi e applicazione della codifica run-length (schema a zig-zag).

**Codifica di Huffman**: si assegnano ai numeri più frequenti codici più brevi di quelli meno frequenti.

![[Linearizzazione.png]]

>Fattore di compressione molto elevato: si può arrivare a rapporti di 20:1 .
>*Per la decodifica si applica l'esecuzione inversa dell'algoritmo, che richiede circa lo stesso tempo.*

---

## MPEG 
> Motion Picture Experts Group.

E' un gruppo di lavoro che dal 1993 si occupa di algoritmi e formati per le immagini in movimento.

**MPEG-1**: pubblicato nel 1993, è ancora molto usato, il suo scopo era quello di produrre un output di qualità simile a quella di un videoregistratore (352x240 per NTSC) usando un bit rate pari a 1,2Mbps con compressione 40:1 (richiederebbe circa 50,7Mbps non compressi).

E’ composto da 3 parti:
- Audio
- Video
- Multiplexer di sistema

![[mpeg.png]]

Audio e video lavorano indipendentemente e sono sincronizzati usando un segnale comune (clock) a 90 KHz che emette il segnale del tempo corrente verso entrambi i codificatori.

- **Ridondanza spaziale**: si usa la codifica JPEG per ogni singolo fotogramma. Utile soprattutto per accedere in modo casuale ad ogni singolo fotogramma.
- **Ridondanza temporale**: si cerca di trarre vantaggio dal fatto che i fotogrammi consecutivi sono quasi identici.

### MPEG-1 vs MPEG-2
MPEG-1 è composto da 3 tipi di fotogrammi: 
1. **I-frame (Intracodificati)**: immagini statiche codificate in JPEG. Vengono inviate ad intervalli regolari (ad esempio ogni 10 frame).
2. **P-frame (Predittivi)**: E' una immagine che dipende dal frame precedente. Viene determinata calcolando le differenze blocco per blocco con l'ultimo fotogramma. Utile se non ci sono troppi cambiamenti rispetto alla precedente.
3. **B-frame (Bidirezionali)**: dipende sia dal precedente che dal successivo. 

![[mpeg1.png]]

MPEG-2: rilasciato nel 1996, è progettato per comprimere video con bitrate compresi tra 4 e 6 Mbps, per essere inserito in trasmissioni NTSC e PAL per poi arrivare anche a risoluzioni superiori (HDTV):
- In prima approssimazione MPEG-2 è un superset di MPEG-1.
- Utilizza fotogrammi I,P,B.
- La trasformata DCT utilizza blocchi 10x10 invece di blocchi 8x8.
- Supporta sia immagini progressive che interallacciate.
- Supporta più livelli di risoluzione: Low (352x240 compatibile MPEG-1 ), Main (720x480) High-1440 (1440x1152), High (1920x1080).

---

# Dati multimediali in rete
Esistono diversi modalità di utilizzo del multimedia in internet, con diverse requisiti di rete: 
- **Streaming di contenuti registrati** (e.g. Youtube, Netflix)
	- Numerosi flussi singoli. Riproduzione durante la ricezione. 
- **Realtime streaming** (e.g. radio Internet, dirette sportive)
	- Ridurre il ritardo per minimizzare lo scostamento temporale rispetto alla trasmissione via etere.
	- Centinaia o migliaia di utenti contemporanei 
- **Conferenza in tempo reale** (e.g. Skype)
	- L’interattività richiede latenze molto contenute.
	- Numero di partecipanti tipicamente limitato ma in alcuni casi può essere elevato.

---

## Download and Play
> Streaming di contenuti registrati.

Il file deve essere scaricato completamente prima di poter essere riprodotto utilizzando il sistema “classico” del MIME (esempio `Content-type: video/mp4`) e delle helper applications.

![[streaming_registrati.png]]

>*Sistema poco efficiente: si deve scaricare il file prima di iniziare l'ascolto.*

## Streaming on Download
> Streaming di contenuti registrati.

La soluzione generalmente adottata (Audio Streaming) è la seguente: il file collegato al titolo non è quello contenente l’audio ma un *metafile* che rimanda al file vero e proprio da ascoltare (il contenuto potrebbe essere la sola riga `rtsp://server/file.mp3`).
Il browser, una volta passato il metafile all’applicazione esterna, non fa più parte del ciclo di comunicazione.

![[streaming_on_download.png]]

L'accesso al file multimediale (e.g. `rtsp://server/file.mp3`) avviene mediante un protocollo per la gestione dell'interfaccia utente (Play/Record/Pause/ ecc) denominato Real Time Streaming Protocol ( RTSP). Il trasporto dl flusso multimediale avviene su un canale separato basato sul protocollo RTP (Real-time Transport Protocol) o HTTP.

### RTSP
*Il protocollo RTSP (Real Time Streaming Protocol) supporta un set di comandi che vengono inviati al server mediante un scambio testuale simile all'HTTP:*

![[RTSP.png]]

> Esempio di dialogo: [Wikipedia](https://en.wikipedia.org/wiki/Real-Time_Streaming_Protocol).

### RTP
**RTP** (Realtime Transport Protocol) è un protocollo client/server per il trasporto di flussi anche multipli di dati audio e video.
È basato su UDP e può funzionare sia in unicast che in multicast. 

I campi principali dell'intestazione RTP sono:
- Payload type: rende possibile l'identificazione del contenuto (esempi: 26 → JPEG, 32 → MPEG1, 33 → MPEG2).
- Sequence Number: numerazione progressiva per il riordino.
- Timestamp: istante di campionamento del primo byte nel payload.
- Synchronized Source ID: identificatore della sorgente di steam, per distinguere diversi flussi contemporanei. 

Con l'aiuto di un piccolo buffer il ricevente può ricostruire il flusso nella sequenza temporale corretta e rifasare eventuali flussi contemporanei.

![[RTP.png]]

> Alcuni fornitori preferiscono implementare protocolli di trasporto brevettati.
> Ad esempio i server di Realnetworks utilizzano il protocollo di trasporto [Real Data Transport (RDT)](https://en.wikipedia.org/wiki/Real_Data_Transport) di proprietà della RealNetworks stessa.

---

## Realtime Streaming
Requisiti: 
- Ridurre il ritardo per minimizzare lo scostamento temporale rispetto alla trasmissione via etere. 
- Centinaia o migliaia di utenti contemporanei.

Soluzioni: 
- Multicast con protocolli RTP/UDP: buona scalabilità, ma sia multicasting che la porta RTP possono incontrare problemi nel supporto da parte dei provider. Vedi [Fastweb multicast](https://www.fastweb.it/fastweb-plus/digital-magazine/protocollo-ssdp-cos-e-e-come-funziona/).
- Dynamic adaptive streaming over HTTP, in *unicast*.

### Dynamic Adaptive Streaming over HTTP
**Dynamic Adaptive Streaming** è una tecnica particolare, basata su HTTP, che adegua la qualità dello streaming video alle risorse disponibili nel dispositivo dell’utente, quali le condizioni della rete (bandwith) o la capacità della CPU, avendo come risultato il miglioramento della QoS dell’utente.

Funziona bene se il numero di utenti è moderato ed è necessario avere il server con buona connettività internet, eventualmente in CDN (Content Distribution Network).

Il funzionamento di un CDN consiste nel memorizzare copie dei contenuti in server distribuiti in diverse posizioni geografiche, chiamati punti di presenza o PoP (Points of Presence). Quando un utente richiede un certo contenuto, il sistema di CDN indirizza la richiesta al server più vicino geograficamente all'utente anziché al server originale del sito web. Questo riduce la latenza e migliora i tempi di caricamento delle pagine, fornendo una migliore esperienza utente.

---

## Conferenze in tempo reale
Requisiti:
- L’interattività richiede latenze molto contenute.
- Numero di partecipanti tipicamente limitato ma in alcuni casi può essere elevato.

- **Latenza**: la rete telefonica considera accettabili latenze in una direzione entro 150 ms Il ritardo di propagazione di una f.o. per 8000 Km (Seattle-Amsterdam) è di 40ms, a cui si aggiungono i ritardi introdotti dai router. 
	- **Ritardo dovuto al riempimento del pacchetto**: un pacchetto da 1KB impiega 125ms per riempirsi a 64Kbps. Se si utilizzano pacchetti piccoli (160 bytes) è possibile ridurre la latenza, accettando un degrado di larghezza di banda. 
	- Altra **latenza è data dal software** (compressione / decompressione), soprattutto per il video. 
- **Buffer**: è ancora necessario per evirare riproduzioni a scatti, ma deve essere piccolo per limitare la latenza. La presenza del buffer consente comunque la gestione dei pacchetti ricevuti fuori ordine mediante l'uso del numero di sequenza. 
- **Protocolli**: Occorre un protocollo per il trasporto dei dati, che generalmente è RTP/UDP. Il protocollo RTCP serve per lo scambio di messaggi di controllo, qualità del servizio, controllo di sessione, ecc.

### VoIP
Il VoIP (Voice over Internet Protocol con H.323) è una tecnologia che consente la trasmissione delle chiamate vocali attraverso Internet anziché tramite le tradizionali linee telefoniche. 

**H.323** è uno dei protocolli utilizzati per implementare il VoIP e si concentra sulla standardizzazione della comunicazione multimediale su reti basate su pacchetti, come Internet. 
Non emette proprie specifiche ma fa riferimento a diversi protocolli specializzati: codifica del parlato, impostazione delle chiamate, segnalazione, trasporto di dati, etc.

![[voip323.png]]

### VoIP con SIP
SIP (Session Initiation Protocol) è la risposta di IETF (RFC 3261) ad H.323, considerato un prodotto tipico per telecomunicazioni (grande, complesso e poco flessibile), con un protocollo più semplice e modulare per la telefonia via Internet.

Descrive come impostare le telefonate via internet, le videoconferenze e altre connessioni multimediali. Gestisce solamente l’impostazione, la gestione e la terminazione delle sessioni.

>*Altri protocolli si occupano del trasporto dei dati. Per il traffico multimediale si usa di preferenza RTP (RFC 3550, RFC 5506).*

#### Architettura SIP
Principali elementi: 
- **User Agent**: è un end-point e può fungere da client o da server 
- **Proxy server**: è un server intermedio; può rispondere direttamente alle richieste oppure inoltrarle ad un client, ad un server o ad un ulteriore proxy. Un proxy server analizza i parametri di instradamento dei messaggi e "nasconde" la reale posizione del destinatario del messaggio, essendo quest'ultimo indirizzabile con un nome convenzionale del dominio di appartenenza. 
- **Location Server**: è un database contenente informazioni sull'utente, come il profilo, l'indirizzo IP, l'URL.

![[sip_arch.png]]

#### Protocollo SIP
**SIP** è un protocollo di segnalazione utilizzato per inizializzare, modificare e terminare sessioni di comunicazione multimediale come chiamate vocali e video su reti IP. È progettato per essere un protocollo leggero ed estensibile.

Un SIP-URI (RFC 3261) rappresenta lo schema di indirizzamento SIP per chiamare un altro soggetto attraverso il protocollo SIP. 

In altre parole, un SIP URI è il recapito telefonico SIP di un utente, assomiglia ad un indirizzo e-mail scritto nel seguente formato: 
`{sip|sips}:[user-part@]domain-part[:port]` 
Esempio: `sip:alfierir@ekiga.net`

Queste URI possono contenere indirizzi IPV4, IPV6 o numeri di telefono veri e propri:
`sips:+004437612234@sip-proxy.org:5062`

Il protocollo è basato su UDP/5060 con transazioni richiesta/risposta in ASCII (simile ad HTTP). 
Una transazione inizia con una Request inviata da uno User Agent Client ad un Proxy e termina con una Final Response inviata in senso inverso.

![[SIP 1.png]]

